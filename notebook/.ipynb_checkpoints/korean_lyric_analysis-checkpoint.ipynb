{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.appName(\"lyric Clustering\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./lyric_ko_big.csv', header=None, names=[\"title\", \"artist\", \"lyric\"])\n",
    "df = df.dropna(axis=0, how='any')\n",
    "df[\"lyric\"] = df.lyric.apply(lambda x: x.replace(\",\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"./lyric_ko_pre.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    ".format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"lyric_ko.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|               title|     artist|               lyric|\n",
      "+--------------------+-----------+--------------------+\n",
      "|             그때 또 다시|         유노|돌이켜보면 너같은 사람 나에게 ...|\n",
      "|      Love Love Love|선비(SunBee..|Are you there my ...|\n",
      "|Butterfly (Prod. ...|  LambC(램씨)|There is a time t...|\n",
      "|            GOOD BAD|       입술세개|24 군대 막 다녀와서 널 만났...|\n",
      "|             너무 원했기에|        천소아|아직 이해가 안 돼소나기처럼 내...|\n",
      "|               이별중이야|데이스타(Days..|욕심이라고 몇 번을 말해야 받아...|\n",
      "|                Time|      V.O.S|감추려 숨기려 지우려고 아무리 ...|\n",
      "|우유부단 (Prod. by K ...|조현아(어반자카파..|두려워 지금 너의 눈빛이물론 짐...|\n",
      "|  기억이 난다 (Feat. 강슬기)|장한종과 J. F..|아련한 안개길 가운데 서니앞에 ...|\n",
      "|                  그냥|    노래안하는사람|알았지만 애써 외면했어인정하면 ...|\n",
      "|                  이유|   구오 (GUO)|그대가 날 떠난다고 생각한 적 ...|\n",
      "|               그대이기에|더 원(The O..|눈부시게 빛나는 그대 얼굴이표정...|\n",
      "|                Dawn|곽키(Quak-E..|저 차가운 밤을깨운 그 다음묶여...|\n",
      "|party (SHUT DOWN)...|Sik-K(식케이..|Groovy Everywhere...|\n",
      "|               그렇게 또|비볼드(Bebol..|넌 그렇게 또장난처럼 내게알수없...|\n",
      "|                 매화향|스쿠터 다이어리(..|그대 입술 향기 맑으니 바람일면...|\n",
      "| 어디로 가나요 (Feat. 신지윤)|파이진(Pyzen..|가슴이 멈쳐지는줄만 같았지왜 날...|\n",
      "|               데이트할까|이루다 (E-ru..|눈부신 아침햇살에부스스 일어나세...|\n",
      "|             Show Me|제이슨 정 (Ja..|소리 없는 이 밤얇은 커튼 사이...|\n",
      "| Save me (Feat. 김환수)|        강희원|쓸쓸한 가로등 불빛그곳에 난 서...|\n",
      "+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19637"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from customTransformer import KonlpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = KonlpTokenizer(inputCol=\"lyric\", outputCol=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokensDF = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+\n",
      "|               title|     artist|               lyric|              tokens|\n",
      "+--------------------+-----------+--------------------+--------------------+\n",
      "|             그때 또 다시|         유노|돌이켜보면 너같은 사람 나에게 ...|[돌이, 보면, 같은, 사람, ...|\n",
      "|      Love Love Love|선비(SunBee..|Are you there my ...|[대체, 하느, 늦는, 건지, ...|\n",
      "|Butterfly (Prod. ...|  LambC(램씨)|There is a time t...|                  []|\n",
      "|            GOOD BAD|       입술세개|24 군대 막 다녀와서 널 만났...|[군대, 다녀와, 만났, 작은,...|\n",
      "|             너무 원했기에|        천소아|아직 이해가 안 돼소나기처럼 내...|[아직, 이해, 소나기, 내게,...|\n",
      "|               이별중이야|데이스타(Days..|욕심이라고 몇 번을 말해야 받아...|[욕심, 말해야, 받아들일, 생...|\n",
      "|                Time|      V.O.S|감추려 숨기려 지우려고 아무리 ...|[감추려, 숨기, 지우려, 애써...|\n",
      "|우유부단 (Prod. by K ...|조현아(어반자카파..|두려워 지금 너의 눈빛이물론 짐...|[두려워, 지금, 눈빛, 물론,...|\n",
      "|  기억이 난다 (Feat. 강슬기)|장한종과 J. F..|아련한 안개길 가운데 서니앞에 ...|[아련한, 개길, 가운데, 서니...|\n",
      "|                  그냥|    노래안하는사람|알았지만 애써 외면했어인정하면 ...|[애써, 외면했, 인정하면, 끝...|\n",
      "|                  이유|   구오 (GUO)|그대가 날 떠난다고 생각한 적 ...|[그대, 떠난, 생각한, 한번,...|\n",
      "|               그대이기에|더 원(The O..|눈부시게 빛나는 그대 얼굴이표정...|[눈부시, 빛나는, 그대, 얼굴...|\n",
      "|                Dawn|곽키(Quak-E..|저 차가운 밤을깨운 그 다음묶여...|[차가운, 깨운, 다음, 묶여,...|\n",
      "|party (SHUT DOWN)...|Sik-K(식케이..|Groovy Everywhere...|[들어가기, 싫어, 없는, 우리...|\n",
      "|               그렇게 또|비볼드(Bebol..|넌 그렇게 또장난처럼 내게알수없...|[장난, 내게, 없는, 말로, ...|\n",
      "|                 매화향|스쿠터 다이어리(..|그대 입술 향기 맑으니 바람일면...|[그대, 입술, 향기, 맑으, ...|\n",
      "| 어디로 가나요 (Feat. 신지윤)|파이진(Pyzen..|가슴이 멈쳐지는줄만 같았지왜 날...|[가슴, 같았, 쳐다보는, 관심...|\n",
      "|               데이트할까|이루다 (E-ru..|눈부신 아침햇살에부스스 일어나세...|[눈부신, 아침햇살, 부스스, ...|\n",
      "|             Show Me|제이슨 정 (Ja..|소리 없는 이 밤얇은 커튼 사이...|[소리, 없는, 얇은, 커튼, ...|\n",
      "| Save me (Feat. 김환수)|        강희원|쓸쓸한 가로등 불빛그곳에 난 서...|[쓸쓸한, 가로등, 불빛, 시작...|\n",
      "+--------------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokensDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"tf\")\n",
    "# tfLyric = hashingTF.transform(tokensDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "# idfModel = idf.fit(tfLyric)\n",
    "# tfidfLyric = idfModel.transform(tfLyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tfidfLyric.select(\"title\", \"tfidf\").show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49525"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "vectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"tf\").fit(tokensDF)\n",
    "voca = vectorizer.vocabulary\n",
    "len(voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vetorizedDF = vectorizer.transform(tokensDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(vetorizedDF)\n",
    "tfidfLyric = idfModel.transform(vetorizedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import ArrayType, StringType\n",
    "# from pyspark.sql.functions import udf\n",
    "\n",
    "# def indices_to_terms(vocabulary):\n",
    "#     def indices_to_terms(xs):\n",
    "#         return [vocabulary[int(x)] for x in xs]\n",
    "#     return udf(indices_to_terms, ArrayType(StringType()))\n",
    "\n",
    "# ldaModel = ldaModel.withColumn(\n",
    "#     \"topics_words\", indices_to_terms(voca)(\"termIndices\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"features\")\n",
    "normLyrics = normalizer.transform(tfidfLyric)\n",
    "\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# kmeans = KMeans().setK(15).setMaxIter(20)\n",
    "# km_model = kmeans.fit(normLyrics)\n",
    "\n",
    "# clustersTable = km_model.transform(normLyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "cost = []\n",
    "\n",
    "for k in range(2, 30):\n",
    "    kmeans = KMeans().setK(k).setMaxIter(20)\n",
    "    km_model = kmeans.fit(normLyrics)\n",
    "    cost.append(km_model.computeCost(normLyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|        12|    2|\n",
      "|         1|    3|\n",
      "|        13|  749|\n",
      "|         6|    4|\n",
      "|         3| 2268|\n",
      "|         5|    2|\n",
      "|         9| 2861|\n",
      "|         4|    2|\n",
      "|         8|    2|\n",
      "|         7|    1|\n",
      "|        10|    2|\n",
      "|        11|13736|\n",
      "|        14|    2|\n",
      "|         2|    1|\n",
      "|         0|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustersTable.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               tfidf|\n",
      "+--------------------+\n",
      "|(30000,[1,101,167...|\n",
      "|(30000,[1,101,167...|\n",
      "|(30000,[1,167,265...|\n",
      "|(30000,[2,76,307,...|\n",
      "|(30000,[2,167,400...|\n",
      "|(30000,[3,15,126,...|\n",
      "|(30000,[3,15,167,...|\n",
      "|(30000,[3,25,167,...|\n",
      "|(30000,[3,36,110,...|\n",
      "|(30000,[3,54,73,1...|\n",
      "|(30000,[3,73,167,...|\n",
      "|(30000,[3,101,133...|\n",
      "|(30000,[3,126,167...|\n",
      "|(30000,[3,144,167...|\n",
      "|(30000,[3,167,180...|\n",
      "|(30000,[3,423,465...|\n",
      "|(30000,[4,69,110,...|\n",
      "|(30000,[4,69,110,...|\n",
      "|(30000,[4,84,167,...|\n",
      "|(30000,[4,110,118...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustersTable.select(\"tfidf\").orderBy(\"tfidf\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\").fit(tokensDF)\n",
    "# vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49525"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vetorizedDF = vectorizer.transform(tokensDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+--------------------+\n",
      "|               title|     artist|               lyric|              tokens|            features|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+\n",
      "|             그때 또 다시|         유노|돌이켜보면 너같은 사람 나에게 ...|[돌이, 보면, 같은, 사람, ...|(49525,[0,3,8,10,...|\n",
      "|      Love Love Love|선비(SunBee..|Are you there my ...|[대체, 하느, 늦는, 건지, ...|(49525,[5,8,12,15...|\n",
      "|Butterfly (Prod. ...|  LambC(램씨)|There is a time t...|                  []|       (49525,[],[])|\n",
      "|            GOOD BAD|       입술세개|24 군대 막 다녀와서 널 만났...|[군대, 다녀와, 만났, 작은,...|(49525,[4,6,9,10,...|\n",
      "|             너무 원했기에|        천소아|아직 이해가 안 돼소나기처럼 내...|[아직, 이해, 소나기, 내게,...|(49525,[0,3,4,12,...|\n",
      "|               이별중이야|데이스타(Days..|욕심이라고 몇 번을 말해야 받아...|[욕심, 말해야, 받아들일, 생...|(49525,[1,2,4,6,1...|\n",
      "|                Time|      V.O.S|감추려 숨기려 지우려고 아무리 ...|[감추려, 숨기, 지우려, 애써...|(49525,[4,5,9,10,...|\n",
      "|우유부단 (Prod. by K ...|조현아(어반자카파..|두려워 지금 너의 눈빛이물론 짐...|[두려워, 지금, 눈빛, 물론,...|(49525,[12,15,17,...|\n",
      "|  기억이 난다 (Feat. 강슬기)|장한종과 J. F..|아련한 안개길 가운데 서니앞에 ...|[아련한, 개길, 가운데, 서니...|(49525,[5,10,14,1...|\n",
      "|                  그냥|    노래안하는사람|알았지만 애써 외면했어인정하면 ...|[애써, 외면했, 인정하면, 끝...|(49525,[0,4,6,9,1...|\n",
      "|                  이유|   구오 (GUO)|그대가 날 떠난다고 생각한 적 ...|[그대, 떠난, 생각한, 한번,...|(49525,[1,4,8,9,1...|\n",
      "|               그대이기에|더 원(The O..|눈부시게 빛나는 그대 얼굴이표정...|[눈부시, 빛나는, 그대, 얼굴...|(49525,[1,3,7,8,9...|\n",
      "|                Dawn|곽키(Quak-E..|저 차가운 밤을깨운 그 다음묶여...|[차가운, 깨운, 다음, 묶여,...|(49525,[4,5,9,11,...|\n",
      "|party (SHUT DOWN)...|Sik-K(식케이..|Groovy Everywhere...|[들어가기, 싫어, 없는, 우리...|(49525,[2,6,7,12,...|\n",
      "|               그렇게 또|비볼드(Bebol..|넌 그렇게 또장난처럼 내게알수없...|[장난, 내게, 없는, 말로, ...|(49525,[7,12,16,5...|\n",
      "|                 매화향|스쿠터 다이어리(..|그대 입술 향기 맑으니 바람일면...|[그대, 입술, 향기, 맑으, ...|(49525,[0,1,36,37...|\n",
      "| 어디로 가나요 (Feat. 신지윤)|파이진(Pyzen..|가슴이 멈쳐지는줄만 같았지왜 날...|[가슴, 같았, 쳐다보는, 관심...|(49525,[13,17,27,...|\n",
      "|               데이트할까|이루다 (E-ru..|눈부신 아침햇살에부스스 일어나세...|[눈부신, 아침햇살, 부스스, ...|(49525,[2,13,19,3...|\n",
      "|             Show Me|제이슨 정 (Ja..|소리 없는 이 밤얇은 커튼 사이...|[소리, 없는, 얇은, 커튼, ...|(49525,[1,2,3,6,7...|\n",
      "| Save me (Feat. 김환수)|        강희원|쓸쓸한 가로등 불빛그곳에 난 서...|[쓸쓸한, 가로등, 불빛, 시작...|(49525,[5,7,9,10,...|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vetorizedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LDA(k=5, seed=1, optimizer=\"em\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldaDF = lda.fit(vetorizedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaDF.isDistributed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49525"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaDF.vocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voca = vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldaModel = ldaDF.describeTopics().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def indices_to_terms(vocabulary):\n",
    "    def indices_to_terms(xs):\n",
    "        return [vocabulary[int(x)] for x in xs]\n",
    "    return udf(indices_to_terms, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldaModel = ldaModel.withColumn(\n",
    "    \"topics_words\", indices_to_terms(voca)(\"termIndices\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|        topics_words|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    1|[0, 2, 6, 7, 3, 5...|[0.00824126830050...|[사랑, 우리, 오늘, 없는, ...|\n",
      "|    4|[0, 2, 1, 3, 6, 5...|[0.01390785978933...|[사랑, 우리, 그대, 마음, ...|\n",
      "|    0|[1, 0, 2, 3, 5, 1...|[0.01218235255123...|[그대, 사랑, 우리, 마음, ...|\n",
      "|    2|[1, 0, 4, 2, 3, 7...|[0.01109427689030...|[그대, 사랑, 시간, 우리, ...|\n",
      "|    3|[1, 0, 3, 2, 4, 1...|[0.01278534852259...|[그대, 사랑, 마음, 우리, ...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldaModel.orderBy(ldaModel.topics_words.desc()).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(vetorizedDF)\n",
    "tfidfLyric = idfModel.transform(vetorizedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-9a60ffb68891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Trains a KMeans model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaxIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mkm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormLyrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mclustersTable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormLyrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark-2.1.0/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/src/spark-2.1.0/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark-2.1.0/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark-2.1.0/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/src/spark-2.1.0/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark-2.1.0/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dobro/anaconda3/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"norm\")\n",
    "normLyrics = normalizer.transform(tfidfLyric)\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Trains a KMeans model.\n",
    "kmeans = KMeans().setK(10).setMaxIter(20)\n",
    "km_model = kmeans.fit(normLyrics)\n",
    "\n",
    "clustersTable = km_model.transform(normLyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
